from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import tensorflow as tf


slim = tf.contrib.slim

VOC_LABELS = {
    'none': (0, 'Background'),
    'aeroplane': (1, 'Vehicle'),
    'bicycle': (2, 'Vehicle'),
    'bird': (3, 'Animal'),
    'boat': (4, 'Vehicle'),
    'bottle': (5, 'Indoor'),
    'bus': (6, 'Vehicle'),
    'car': (7, 'Vehicle'),
    'cat': (8, 'Animal'),
    'chair': (9, 'Indoor'),
    'cow': (10, 'Animal'),
    'diningtable': (11, 'Indoor'),
    'dog': (12, 'Animal'),
    'horse': (13, 'Animal'),
    'motorbike': (14, 'Vehicle'),
    'person': (15, 'Person'),
    'pottedplant': (16, 'Indoor'),
    'sheep': (17, 'Animal'),
    'sofa': (18, 'Indoor'),
    'train': (19, 'Vehicle'),
    'tvmonitor': (20, 'Indoor'),
    'iu': (21, 'face')
}

data_splits_num = {  #데이터셋의 각 사용할 수를 나눔
    'train': 22136,
    'val': 4952,
}

def slim_get_batch(num_classes, batch_size, split_name, file_pattern, num_readers, image_preprocessing_fn, anchor_encoder, num_epochs=None,
                   is_training=True):
    """
    Gets dataset tuple with instruction for reading Pascal VOC datset.

    :param num_classes: Total class numbers in dataset
    :param batch_size: the size of each batch
    :param split_name: 'train' of 'val'
    :param file_pattern: the file pattern to use when matching the dataset sources (full path)
    :param num_readers: the max number of reader used for reading tfrecords.
    :param image_processing_fn: the function used to dataset argumentation.
    :param anchor_encoder: the function used to encoder all anchors
    :param num_epochs: totoal epoches for iterate this dataset
    :param is_training: whether we are in training phase.
    :return: A batch of [image, shape, loc_targets, cls_targets, match_scores].
    """

    if split_name not in data_splits_num:
        raise ValueError('split name %s was not recognized. '% split_name)

    # Features in Pascal VOC TFRecords. (XML file format에 속해있는것)
    keys_to_features = {
        'image/encoded' : tf.FixedLenFeature((), tf.string, default_value=''),
        'image/format' : tf.FixedLenFeature((), tf.string, default_value='jpeg'),
        'image/filename' : tf.FixedLenFeature((), tf.string, default_value=''),
        'image/height' : tf.FixedLenFeature([1],tf.int64),
        'image/width' : tf.FixedLenFeature([1], tf.int64),
        'image/channels' : tf.FixedLenFeature([1], tf.int64),
        'image/shape' : tf.FixedLenFeature([3], tf.int64),
        'image/object/bbox/xmin':tf.VarLenFeature(dtype=tf.float32),
        'image/object/bbox/ymin': tf.VarLenFeature(dtype=tf.float32),
        'image/object/bbox/xmax': tf.VarLenFeature(dtype=tf.float32),
        'image/object/bbox/ymax': tf.VarLenFeature(dtype=tf.float32),
        'image/object/bbox/label': tf.VarLenFeature(dtype=tf.int64),
        'image/object/bbox/difficult': tf.VarLenFeature(dtype=tf.int64),
        'image/object/bbox/truncated': tf.VarLenFeature(dtype=tf.int64),
    }

    items_to_handlers = {
        'image ' : slim.tfexample_decoder.image('image/encoded', 'image/format'),
        'filename':slim.tfexample_decoder.Tensor('image/filename'),
        'shape' : slim.tfexample_decoder.Tensor('image/shape'),
        'object/bbox' : slim.tfexample_decoder.BoundingBox(['ymin', 'xmin', 'ymax', 'xmax'], 'image/object/bbox/'),
        'object/label' : slim.tfexample_decoder.Tensor('image/object/bbox/difficult'),
        'object/truncated': slim.tfexample_decoder.Tensor('image/object/bbox/truncated'),
    }

    decoder = decoder = slim.tfexample_decoder.TFExampleDecoder(keys_to_features, items_to_handlers)

    labels_to_names = {}
    for name, pair in VOC_LABELS.items():
        labels_to_names[pair[0]] = name     #딕셔너리 구조 Key : value


    dataset = slim.dataset.Dataset(
        data_sources = file_pattern,
        reader = tf.TFRecordReader,
        decoder=decoder,
        num_samples = data_splits_num[split_name],
        items_to_descriptions = None,
        num_classes=num_classes,
        abels_to_names=labels_to_names
    )

    with tf.name_scope('dataset_data_provider'):
        provider = slim.dataset_data_provider.DatasetDataProvider(
            dataset,
            num_readers=num_readers,
            shuffle=is_training,
            num_epochs=num_epochs
        )

    [org_image, filename, shape, glabels_raw, gbboxes_raw, isdifficult] = provider.get(['image', 'filename', 'shape',
        'object/label', 'object/bbox',' object/difficult'])

    if is_training :
    # if all is difficult, then keep the first one
        isdifficult_mask = tf.cond(tf.count_nonzero(isdifficult, dtype=tf.int32) < tf.shape(isdifficult)[0],
                                lambda : isdifficult < tf.ones_like(isdifficult),
                                lambda : tf.one_hot(0, tf.shape(isdifficult)[0], on_value=True, off_value=False, dtype=tf.bool))

        glabels_raw = tf.boolean_mask(glabels_raw, isdifficult_mask)
        gbboxes_raw = tf.boolean_mask(gbboxes_raw, isdifficult_mask)


    #pre-processing image, labels and bboxes.
    if is_training:
        image, glabels, gbboxes = image_preprocessing_fn(org_image, glabels_raw, glabels_raw)
    else:
        image = image_preprocessing_fn(org_image, glabels_raw, gbboxes_raw)
        glabels, gbboxes = glabels_raw, gbboxes_raw

    gt_targets, gt_labels, gt_scores = anchor_encoder(glabels, gbboxes)

    return tf.train.batch([image, filename, shape, gt_targets, gt_labels, gt_scores],
                          dynamic_pad=False,
                          batch_size=batch_size,
                          allow_smaller_final_batch=(not is_training),
                          )
